{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture 3 Dyanamic Programming",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNSrZEIoxql_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Implementing policy evaluation and policy iteration based\n",
        "#on David Silver's RL Lecture Series Part 3\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtPOppWYy6Ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_value_function(env,state,V,discount):\n",
        "\n",
        "  #This function will calculate the value function using one step lookahead\n",
        "\n",
        "  action_values = np.zeros(env.nA)\n",
        "\n",
        "  for action in range(env.nA):\n",
        "\n",
        "    #Iterating over and applying the Bellman Equation with one step lookahead.\n",
        "    for prob, next_state,reward,terminated in env.P[state][action]:\n",
        "      action_values[action] += prob*(reward + discount*V[next_state])\n",
        "      \n",
        "\n",
        "  return action_values\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SAc6LXw-Ii2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_evaluation(policy,env,discount,cf = 1e-9,max_iterations = 1e9):\n",
        "  \n",
        "  iteration = 1\n",
        "\n",
        "  V = np.zeros(env.nS)\n",
        "\n",
        "  for i in range(int(max_iterations)):\n",
        "\n",
        "    delta = 0\n",
        "\n",
        "    for state in range(env.nS):\n",
        "\n",
        "      v = 0\n",
        "\n",
        "      for action,action_prob in enumerate(policy[state]):\n",
        "        for state_prob,next_state,reward,terminated in env.P[state][action]:\n",
        "\n",
        "          v+=action_prob*state_prob*(reward + discount*V[next_state])\n",
        "\n",
        "      delta = max(delta,abs(V[state]-v))\n",
        "\n",
        "      V[state] = v\n",
        "\n",
        "      iteration+=1\n",
        "\n",
        "      if (delta<cf):\n",
        "        print(\"Policy evaluated\")\n",
        "\n",
        "        return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkuEtr8kBJP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_iteration(env,discount,max_iteraions = 1e9):\n",
        "  \n",
        "\n",
        "  policy = np.ones((env.nS,env,nA)) / env.nA\n",
        "\n",
        "  evaluated_policies = 1\n",
        "\n",
        "  for i in range(int(max_iterations)):\n",
        "\n",
        "    stable_policy = True\n",
        "\n",
        "    V = policy_evaluation(policy,env,discount)\n",
        "\n",
        "    for state in range(env.nS):\n",
        "\n",
        "      current_action = np.argmax(policy[state])\n",
        "\n",
        "      action_values = calc_value_function(env,state,V,discount)\n",
        "\n",
        "      best_action = np.argmax(action_values)\n",
        "\n",
        "      if(current_action != best_action):\n",
        "        stable_policy = False\n",
        "\n",
        "      policy[state] = np.eye(env.nA)[best_action]\n",
        "\n",
        "  evaluated_policies +=1\n",
        "\n",
        "\n",
        "  if(stable_policy):\n",
        "\n",
        "    return policy, V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHWzO_AWCqOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_iteration(env,discount,cf = 1e-9,max_iterations = 1e9):\n",
        "\n",
        "  V = np.zeros(env.nS)\n",
        "  \n",
        "  for i in range(max_iterations):\n",
        "\n",
        "    delta = 0 \n",
        "\n",
        "    for state in range(env.nS):\n",
        "\n",
        "      action_values = calc_value_function(env,state,V,discount)\n",
        "\n",
        "      best_action_value = np.max(action_values)\n",
        "\n",
        "      delta = max(delta, abs(V[state] - best_action_value))\n",
        "\n",
        "      V[state] = best_action_value\n",
        "\n",
        "    if(delta<cf):\n",
        "      break\n",
        "\n",
        "    \n",
        "  policy = np.zeros((env.nS,env.nA))\n",
        "\n",
        "  for state in range(env.nS):\n",
        "    \n",
        "     action_values = calc_value_function(env,state,V,discount)\n",
        "\n",
        "     best_action = np.max(action_values)\n",
        "\n",
        "     policy[state][action] = 1.0\n",
        "\n",
        "\n",
        "  return policy,V\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTvch3y1GZ8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}